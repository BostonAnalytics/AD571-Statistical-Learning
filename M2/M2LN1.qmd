---
title: "Multiple Linear Regression"
subtitle: "Ames Housing Data"
date: 09/29/2023
date-modified: last-modified
date-format: long
author:
  - name: Nakul R. Padalkar
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
format:
  html:
    theme: [cosmo, theme/theme.scss]
    toc: true
    code-overflow: wrap
    number-sections: true
    self-contained: true
bibliography: ../references.bib
tbl-cap-location: top
crossref:
  fig-title: '**Figure**'
  fig-labels: '**arabic**'
  title-delim: "**:**"
  tbl-title: '**Table**'
  tbl-labels: '**arabic**'
editor: 
  markdown: 
    wrap: 65
knitr:
  opts_chunk: 
    collapse: false
    comment: "#>"
    fig.path: "figure/"
    fig.align: center
    dev: "png"
    echo: true
    message: false
    warning: true
    tidy: false
    tidy.opts: 
      blank: false
      width.cutoff: 60
      wrap: 60
---

# Setting up the environment


```{r importlibs, message=FALSE, warning=FALSE}
library(gdata)
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
library(kableExtra)
library(tidyverse)
library(stringr)
library(lubridate)
library(scales)
library(graphics)
library(caret)
```

# Introduction

## Background

The Ames Housing dataset, an improvement over the well-known Boston Housing dataset, has become an essential tool for budding data scientists and researchers focusing on regression problems. Comprising 2930 observations and many explanatory variables ranging from the quality of streets to the number of fireplaces, it paints a detailed picture of the residential homes in Ames, Iowa.

## Objective
This analysis aims to construct a predictive model that, using the various features of a home, accurately predicts its sale price. Such a model has practical applications for various stakeholders in the real estate market.

## Importance

With a robust prediction model, potential homeowners can gauge if a listing is over or underpriced, sellers can ensure they're placing a competitive price, and real estate professionals can provide evidence-backed advice to clients.

## Methodology Overview
Our approach is systematic:

1.  Data Exploration - Getting familiar with the data's structure.
2.  Data Preprocessing - Preparing data for modeling.
3.  Feature Engineering - Optimizing representation of the predictors.
4.  Feature Selection - Using forward selection, backward elimination, and best subset selection.
5.  Model Selection & Training - Choosing an algorithm and training it.
6.  Model Evaluation - Validating the model's accuracy.
7.  Prediction & Conclusion - Making predictions and concluding the analysis.


# Data Exploration in Ames Housing Dataset

## Loading the Data

This section focuses on loading the necessary libraries and the dataset for our exploration.

```{r load_libraries, echo=TRUE}
# Loading the required libraries
library(tidyverse)
library(corrplot)
```

```{r load_data, echo=TRUE}
# Loading the Ames Housing dataset
ames_data <- read.csv("../data/AmesHousing2.csv")
ggplot(data = ames_data, aes(x = Gr.Liv.Area, y = SalePrice)) +
  geom_point(color='#336699') +
  labs(x = "Ground Living Area", y = "Sale Price") +
  ggtitle("Scatterplot of Sale Price vs. Gr.Liv.Area")
```

```{r load_partial_data, echo=TRUE}
# Loading the Ames Housing dataset
filtered_ames_data <- ames_data[ames_data$Gr.Liv.Area <= 4000, ]
ggplot(data = filtered_ames_data, aes(x = Gr.Liv.Area, y = SalePrice)) +
  geom_point(color='#336699') +
  labs(x = "Ground Living Area", y = "Sale Price") +
  ggtitle("Scatterplot of Sale Price vs. Gr.Liv.Area")
```

## Basic Overview

Before diving deeper into the data, it's essential to get an overall sense of its structure, summary statistics, and a peek into the first few rows.

### View the first few rows of the dataset

```{r basic_head}
library(knitr)
library(kableExtra)

# Create a scrollable kable
kable(ames_data[1:10, ], "html") %>%
  kable_styling("striped", full_width = T, position = "center") 
    # scroll_box(width = "750px", height = "300px")
```

### Display the structure of the dataset

```{r basic_overview, echo=TRUE}
# str(ames_data)
library(skimr)
library(gt)

ames_data |> 
  skimr::skim() |>
  gt::gt()
```

### Display summary statistics for the dataset

```{r basic_summ}
# Install and load the required library if not already done
library(gt)

summary(ames_data)

```


## Histogram of SalePrice

To understand the distribution of our target variable, `SalePrice`, we'll visualize it using a histogram.

```{r histogram_saleprice, echo=TRUE, fig.width=8, fig.height=6}
# Plotting a histogram for SalePrice
ggplot(data = ames_data, aes(x = SalePrice)) +
  geom_histogram(fill = '#336699', color = "black", bins = 50) +
  labs(title = "Histogram of SalePrice", x = "SalePrice", y = "Frequency") +
  theme_minimal()
```

## Correlation Analysis

### Correlation with respect to SalePrice

First, let's identify how the continuous variables in our dataset correlate with `SalePrice`.

```{r correlation_saleprice, echo=TRUE, fig.width=8, fig.height=6}
# Calculating correlations of continuous variables with SalePrice
continuous_vars <- ames_data %>% select_if(is.numeric)
correlations <- cor(continuous_vars)
saleprice_correlations <- correlations['SalePrice',]
saleprice_correlations_df <- data.frame(Variable = names(saleprice_correlations), 
                                        Correlation = saleprice_correlations) %>%
  arrange(desc(Correlation))

# Plotting these correlations
ggplot(data = saleprice_correlations_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Correlation of Continuous Variables with SalePrice", x = "", y = "Correlation") +
  theme_minimal()
```

### Correlation among Continuous Variables

Understanding how continuous variables correlate with each other can provide insights into potential multicollinearity and relationships between variables.

```{r correlation_continuous_fix, echo=TRUE, fig.width=12, fig.height=12}
# Remove columns with zero variance or too many NAs
filtered_continuous_vars <- continuous_vars %>% 
  select_if(function(x) var(x, na.rm = TRUE) > 0 & mean(is.na(x)) < 0.5)

# Recalculate correlations
fixed_correlations <- cor(filtered_continuous_vars, use = "complete.obs")

# Replace NA with 0 in correlation matrix
fixed_correlations[is.na(fixed_correlations)] <- 0

# Plotting correlations among continuous variables
corrplot(fixed_correlations, method = "color", order = "hclust", addCoef.col = "black", tl.cex = 0.75, tl.srt = 45, type= 'lower',number.cex= 0.60)
```

# Data Preprocessing

Data preprocessing in the context of the Ames Housing dataset is a crucial phase where raw data is cleaned and transformed to make it suitable for modeling. It entails addressing missing values, which might indicate absent housing features, ensuring numerical attributes are on a consistent scale through feature scaling, and converting categorical attributes into a machine-readable format using encoding techniques. Additionally, the dataset might benefit from feature engineering, where new attributes are derived or existing ones are modified to better capture underlying patterns in the housing market. Lastly, a train-test split is performed, partitioning the dataset to ensure models can be trained on one subset and validated on another, allowing for an assessment of their real-world performance.

1. Handling Missing Values: In the Ames Housing dataset, missing values might indicate the absence of a feature in a house (like a garage). We can impute these missing values using statistical methods, like replacing with the median for numerical features or the mode for categorical ones, ensuring our models have complete data to learn from.
2. Feature Scaling: In datasets like Ames Housing, with numerous numerical features like area or age, feature scaling ensures all numerical attributes have the same scale. This is essential for algorithms like gradient descent or k-NN, where larger-scale features can disproportionately impact the model.
3. Encoding Categorical Variables: The Ames dataset contains categorical variables, like neighborhood or house style. Encoding transforms these text-based categories into numerical format, often using techniques like one-hot encoding. This conversion ensures machine learning algorithms, which require numerical input, can process this data.
4. Feature Engineering: Feature engineering involves creating new features from the existing ones in the Ames dataset or modifying current attributes to represent the underlying patterns better. For instance, combining '1st Floor Area' and '2nd Floor Area' to get 'Total Floor Area' might give a model a more consolidated view of a property's size.
5. Train-Test Split: To assess the performance of our models on unseen data, we divide the Ames dataset into a training set (to train the model) and a testing set (to test its predictions). A standard split might allocate 70% of the data for training and the remaining 30% for testing, ensuring our models are well-trained and well-validated.

## Handling Missing Values

```{r hand_missing_val}
# List columns with missing values
missing_cols <- colnames(ames_data)[colSums(is.na(ames_data)) > 0]
```

For simplicity, we'll impute missing values with the median for numeric columns and the mode for categorical columns

```{r mising_impute}
for (col in missing_cols) {
  if (is.numeric(ames_data[[col]])) {
    ames_data[[col]][is.na(ames_data[[col]])] <- median(ames_data[[col]], na.rm = TRUE)
  } else {
    mode_val <- as.character(names(sort(table(ames_data[[col]]), decreasing = TRUE)[1]))
    ames_data[[col]][is.na(ames_data[[col]])] <- mode_val
  }
}
```

## Feature Scaling - Normalize numeric features

```{r normalize_conti}
# Not this time
# numeric_cols <- sapply(ames_data, is.numeric)
# ames_data[numeric_cols] <- lapply(ames_data[numeric_cols], scale)
```

## Encoding Categorical Variables

```{r encoding_cat}
# Not Needed for Now
# ames_data <- model.matrix(~ . - 1, data = ames_data)
```

## Feature Engineering

We won't perform feature engineering here for simplicity, but this is where you'd create or modify features based on domain knowledge or insights from EDA.

# 5. Train-Test Split

```{r trainsplit}
set.seed(123) # For reproducibility
sample_index <- sample(seq_len(nrow(ames_data)), size = 0.7 * nrow(ames_data))
train_data <- ames_data[sample_index, ]
test_data <- ames_data[-sample_index, ]
```

# Feature Selection

## Simple Linear Regression Model

Fitting a simple linear model between `SalePrice` and `Gr.Liv.Area` can be achieved using R's `lm()` function.

Here's how you can create and summarize such a model:

```{r slr}
options(scipen=999)
# Fit the linear model on the training data
lm_model <- lm(SalePrice ~ Gr.Liv.Area, data = train_data)

# Summarize the model
# stargazer::stargazer(lm_model, type = "html", title = "Simple Linear Model", ci=TRUE, single.row = FALSE, no.space = FALSE, align = TRUE, digits=5, font.size = "small",  report = "vc*stp")
summary(lm_model)
```

The model above tells you that for one unit increase of `Gr.Liv.Area` Sale Price increases by $`r lm_model$coefficients[1]` + `r lm_model$coefficients[2]`= `r lm_model$coefficients[1]+lm_model$coefficients[2]`$ indicated by the equation $SalePrice = `r lm_model$coefficients[1]` + `r lm_model$coefficients[2]` \times Gr.Liv.Area$.

## Multiple Linear Regression Model

Let's run a multiple linear regression model using all available predictors in the `train_data` to predict `SalePrice`.

Here's how you can create and summarize such a model:

```{r full_mode, results='asis'}
# Fit the multiple linear regression model on the training data using all predictors
full_lm_model <- lm(SalePrice ~ ., data = train_data)

# Summarize the model
stargazer::stargazer(full_lm_model, type = "html", title = "Full Linear Model", ci=TRUE, single.row = T, no.space = FALSE, align = TRUE, digits=5, font.size = "small",  report = "vc*stp")
```

### Regression Diagnostic

Once a linear regression model is fitted, there are four primary assumptions that we should check:

1. Linearity: The relationship between the predictors and the response variable should be linear.
2. Independence: The residuals should be independent, especially in time series data.
3. Homoscedasticity (Constant Variance): The variance of the residuals should be constant across all levels of the independent variables.
4. Normality: The residuals should be approximately normally distributed.
We commonly use diagnostic plots to diagnose potential violations of these assumptions in R. Here's a breakdown of these plots and their descriptions:

#### Residual vs. Fitted (or Predicted) Values:
This plot helps check the assumptions of linearity and equal variance (homoscedasticity).

**Description**: A horizontal band around the 0 reference line indicates that the relationship is linear and the variances are equal. Patterns or trends suggest violations.

```{r resplot,  fig.align='center'}
plot(full_lm_model, which=1)
```

#### Normal Q-Q Plot:
This plot checks the assumption of normality of the residuals.

**Description**: If the residuals are normally distributed, they should fall on a roughly straight line at a 45-degree angle. Deviations from this line indicate departures from normality.

```{r qqplot,  fig.align='center'}
plot(full_lm_model, which=2)
```

#### Scale-Location (or Spread-Location) Plot:
This plot helps check the assumption of equal variance (homoscedasticity).

**Description**: A horizontal band with equally spread points indicates equal variances. A funnel shape (either narrow at the bottom or top) suggests that the variances change with the fitted values, violating the assumption.

```{r scaleloc,  fig.align='center'}
plot(full_lm_model, which=3)
```

#### Residuals vs. Leverage:

This plot helps identify influential case observations that have an undue influence on the regression equation.

**Description**: Points that stand out, especially those in the plot's top right or bottom right, are influential for the regression equation. The Cook's distance lines can help identify significant observations.

```{r reslevplot,  fig.align='center'}
plot(full_lm_model, which=5)
```

Remember, no real-world data will perfectly meet all these assumptions. The key is to identify major deviations that could bias the regression results. If any of these assumptions appear to be violated, further investigation and potentially other modeling approaches or transformations might be necessary.

#### Cook's Distance

Cook's Distance is a measure used in regression analysis to identify influential data points. An influential data point is an observation or set of observations that notably affects the regression equation. Such points can exert undue leverage on the estimated regression coefficients, potentially skewing the model.

**Concept**: Cook's Distance combines the leverage (how extreme the input data is) and the residuals (how extreme the output is) into a single measure.

The measure quantifies how much all the fitted values would change if we excluded a particular observation from the dataset.

**Calculation**: It is computed for each observation and represents the scaled change in the fitted values when the observation is left out of the regression. Mathematically, it's a function of each observation's residual and leverage.

**Interpretation**: A common rule of thumb is that any observation with a Cook's Distance more significant than one might be influential. However, this threshold might be too strict in practice, especially for larger datasets.

Another common practice is to look for points whose Cook's Distance exceeds four times the mean of all Cook's Distance values.

A large Cook's Distance value indicates that the observation strongly influences the regression coefficients. Removing this observation would significantly change the estimated regression line.

It's crucial not to automatically exclude data points based on Cook's Distance alone. If an observation has a high Cook's Distance, one should investigate the reasons for its influence. It could result from data entry or measurement errors or represent a legitimate, interesting outlier.

```{r cookplot, fig.align='center'}
plot(full_lm_model, which=4)
```

## Variable Selection Techniques in Multiple Regression

### Forward Selection:
- **Starting Point**: Begins with no predictors in the model.
- **Procedure**: In each step, the predictor that results in the lowest residual sum of squares (RSS) when added to the model is included. 
- **Stopping Point**: Continues until a pre-specified stopping rule is met, like a certain number of variables, or until adding predictors no longer improves the model by a significant amount.
- **Advantages**: Computationally efficient compared to best subset selection, especially when the number of predictors is large.

```{r frwsel,results='asis'}
# Load necessary library
library(MASS)

# Fit the base model (with only the intercept)
base_model <- lm(SalePrice ~ 1, data=train_data)

# Perform forward selection
forward_selected_model <- step(base_model, 
                               scope=list(lower=~1, 
                                          upper=~Order + Lot.Frontage + Lot.Area + Lot.Shape + Utilities + 
                                                  Lot.Config + Land.Slope + Neighborhood + Bldg.Type + House.Style + Overall.Qual + Overall.Cond + Year.Built + Year.Remod.Add + Foundation + Bsmt.Unf.SF + Total.Bsmt.SF + BaseLivArea + 
                                                  Central.Air + X1st.Flr.SF + X2nd.Flr.SF + Gr.Liv.Area + Full.Bath + Half.Bath + Bathrooms + Bedroom.AbvGr + Kitchen.AbvGr + TotRms.AbvGrd + Fireplaces + Garage.Type + 
                                                  Garage.Area + Wood.Deck.SF + Open.Porch.SF + Enclosed.Porch + 
                                                  X3Ssn.Porch + Screen.Porch + Mo.Sold + Yr.Sold + Sale.Condition),
                               direction="forward",
                               trace=0)  # trace=1 provides a step-by-step output

# View the final model
stargazer::stargazer(forward_selected_model, type = "html", out = "regression.html" ,title = "Simple Linear Model", ci=TRUE, single.row = TRUE, no.space = FALSE, align = TRUE, digits=5, font.size = "small",  report = "vc*stp")

```

### Backward Elimination:
- **Starting Point**: Begins with all predictors in the model.
- **Procedure**: In each step, the predictor that is least significant (i.e., has the highest p-value) and does not contribute significantly to the model fit is removed.
- **Stopping Point**: Continues until a stopping rule is met, often when all remaining predictors are significant at a specified alpha level.
- **Advantages**: Like forward selection, it's more computationally efficient than best subset selection.

```{r backelim, results='asis'}
# Load necessary library
library(MASS)

# Fit the full model (with all predictors)
full_model <- lm(SalePrice ~ Order + Lot.Frontage + Lot.Area + Lot.Shape + Utilities + 
                 Lot.Config + Land.Slope + Neighborhood + Bldg.Type + House.Style + 
                 Overall.Qual + Overall.Cond + Year.Built + Year.Remod.Add + 
                 Foundation + Bsmt.Unf.SF + Total.Bsmt.SF + BaseLivArea + 
                 Central.Air + X1st.Flr.SF + X2nd.Flr.SF + Gr.Liv.Area + Full.Bath + 
                 Half.Bath + Bathrooms + Bedroom.AbvGr + Kitchen.AbvGr + 
                 TotRms.AbvGrd + Fireplaces + Garage.Type + 
                 Garage.Area + Wood.Deck.SF + Open.Porch.SF + Enclosed.Porch + 
                 X3Ssn.Porch + Screen.Porch + Mo.Sold + Yr.Sold + Sale.Condition, 
                 data=train_data)

# Perform backward selection
backward_selected_model <- step(full_model, 
                               direction="backward",
                               trace=0)  # trace=1 provides a step-by-step output

# View the final model
stargazer::stargazer(backward_selected_model, type = "html", out = "regression.html" ,title = "Simple Linear Model", ci=TRUE, single.row = TRUE, no.space = FALSE, align = TRUE, digits=5, font.size = "small",  report = "vc*stp")

```

### Best Subset Selection:
- **Procedure**: Considers all possible combinations of predictors and fits a model for each combination. 
- **Selection Criteria**: The "best" model is selected based on a criterion like RSS, Akaike information criterion (AIC), Bayesian information criterion (BIC), or adjusted R-squared.
- **Advantages**: Can find the optimal model based on the selected criterion.
- **Disadvantages**: Computationally intensive, especially as the number of predictors grows. For example, with p predictors, there are 2^p possible models to evaluate. For this reason, it's often not feasible for datasets with many predictors. For this dataset, I am keeping the maximum number of variables in the model to be `r ncol(train_data)-7`, seven variables less than the entire dataset. Adjusting this number in the code below will make selecting max variables in the model easy.

```{r bestset, results='asis'}
# Load necessary libraries
library(leaps)
library(tidyverse)

# Perform best subset selection
# best_subsets <- regsubsets(SalePrice ~ Order + Lot.Frontage + Lot.Area + Lot.Shape + Utilities + Lot.Config + Land.Slope + Neighborhood + Bldg.Type + House.Style + 
#                            Overall.Qual + Overall.Cond + Year.Built + Year.Remod.Add + Foundation + Bsmt.Unf.SF + Total.Bsmt.SF + BaseLivArea + Central.Air + X1st.Flr.SF + X2nd.Flr.SF + Gr.Liv.Area + Full.Bath + 
#                            Half.Bath + Bathrooms + Bedroom.AbvGr + Kitchen.AbvGr + 
#        TotRms.AbvGrd + Fireplaces + Garage.Type + Garage.Area + Wood.Deck.SF + Open.Porch.SF + Enclosed.Porch + X3Ssn.Porch + Screen.Porch + Mo.Sold + Yr.Sold + Sale.Condition, 
#                            data=train_data, nvmax=ncol(train_data)-7,really.big=T)

# # Extracting the summary
# best_subsets_summary <- summary(best_subsets)

# # Displaying the best model for each number of predictors based on RSS (or adjust for other criteria)
# data.frame(
#   predictors = 1:(ncol(train_set)-1),
#   adjr2 = best_subsets_summary$adjr2,
#   rss = best_subsets_summary$rss,
#   cp = best_subsets_summary$cp,
#   bic = best_subsets_summary$bic
# ) %>% 
#   arrange(desc(adjr2)) %>% 
#   head(1)

# # Identify the best model size based on highest adjusted R-squared
# best_model_size <- which.min(best_subsets_summary$bic)

# # Extract the variables included in the best model
# best_model_vars <- names(which(summary_results$which[best_model_size, ]))

# # Construct the formula
# best_formula <- as.formula(paste("y ~", paste(best_model_vars[-1], collapse = " + ")))

# print(best_formula)
```

In practice, the choice of method depends on:
- The number of predictors (with many predictors, the best subset becomes computationally infeasible).
- The primary goal (prediction accuracy vs. interpretability).
- Computational resources.

While best subset selection can find the most optimal model, forward and backward selection are often preferred due to their computational efficiency and the ability to produce more straightforward, more interpretable models.

We need to consider a selection criterion to compare the best models from forward, backward, and best subset selection methods. The AIC (Akaike Information Criterion) is a commonly used criterion that penalizes the addition of unnecessary predictors to a model. A lower AIC suggests a better-fitting model. Another criterion you could use is the adjusted $R^2$, which adjusts $R^2$ based on the number of predictors in the model. A higher adjusted $R^2$ suggests a better-fitting model.

Let's Extract the best models from all the procedures.

```{r modelmatric}
library(MASS)
library(leaps)

# Forward Selection
forward_aic <- AIC(forward_selected_model)

# Backward Selection
backward_aic <- AIC(backward_selected_model)

# Best Subset Selection

# best_model_size <- which.min(best_subset_summary$cp) # Replace cp with bic or adjr2 as needed
# best_formula <- as.formula(best_subsets$call[[2]][[2]][[best_model_size]])
# best_subset_selected_model <- lm(best_formula, data=train_data)
# best_subset_aic <- AIC(best_subset_selected_model)

# Comparing AIC values to identify the best model
# aic_values <- c(Forward=forward_aic, Backward=backward_aic, BestSubset=best_subset_aic)
aic_values <- c(Forward=forward_aic, Backward=backward_aic)
best_method <- names(which.min(aic_values))

```

The best model selection method based on AIC is `r best_method`

# Prediction Over Test

```{r predmodels,}

# Predictions using the backward_selected_model
predicted_values_forward <- predict(forward_selected_model, newdata = test_data)

# Compare predicted values to actual SalePrice values in the test set
actual_values_test <- test_data$SalePrice
comparison <- data.frame(Actual = actual_values_test, Predicted = predicted_values_forward)

# Calculate Mean Squared Error
mse_forward <- mean((predicted_values_forward - actual_values_test)^2)
rmse <- sqrt(mse_forward)

# Display the MSE and first few rows of the comparison

head(comparison) %>% kbl(fullwidth=F)

```

The Root Mean Squared Error for the Forward Selection Model is `r round(rmse, 4)`.

## Visualizations 

```{r plotdata, fig.width=9.5, fig.height=4.75}
library(ggplot2)

# Plotting actual vs. predicted values
p <- ggplot(comparison, aes(x=Actual, y=Predicted)) +
  geom_point(aes(color = abs(Actual - Predicted)), size = 3, alpha = 0.7) +  # Color points by residual magnitude
  geom_abline(intercept=0, slope=1, linetype="dashed", color="red") +        # Line of perfect prediction
  scale_color_viridis_c(option = "E", direction = -1) + 
  labs(
    title = "Actual vs Predicted SalePrice",
    x = "Actual SalePrice",
    y = "Predicted SalePrice",
    color = "Residual Magnitude"
  ) +
  theme_minimal()
p

# full_lm_model <- lm(cnt ~ .-causal-registerd, data = train_data)

```


# Equations

## Causal 

$$
\begin{align}
ln(y) &= \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2} \implies y= e^{\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}}\\
\sqrt{y} &=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\implies y = (\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2})^2== b_0^2+b_1^2+2*b1b2\\
\sqrt[3]{y} &=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\\
cnt &= 161.807 +85.5765 *temp +	314.3430 *atemp - 275.1803 *hum+ 43.000* windspeed\\
log(cnt) &= 1.9205 +	0.043330139 *temp +	1.287265532 *atemp - 0.979846625 *hum+ * windspeed\\
then\; my\; Y\; is
\end{align}
$$


Intercept	1.92058028
temp	0.043330139
atemp	1.287265532
hum	-0.979846625
windspeed	0.246232091

